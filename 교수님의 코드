!pip install ultralytics yt-dlp

from ultralytics import YOLO
import glob
import cv2
import numpy as np
from IPython.display import Video
import shutil
import time

# yaml ìˆ˜ì • (í•µì‹¬ ë¬¸ì œ í•´ê²°)
yaml_fix = '''path: /content/dataset
train: train/images
val: valid/images
names:
  0: lane
  1: traffic_sign
nc: 2'''

with open('/content/dataset/dataset_fixed.yaml', 'w') as f:
    f.write(yaml_fix)

print("ğŸš€ TensorRT ìµœì í™” YOLO ì¶”ë¡  ì‹œì‘!")
print("="*60)

# 1ï¸âƒ£ ê¸°ë³¸ ëª¨ë¸ë“¤ ë¡œë“œ
print("ğŸ¤– ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ ì¤‘...")
base_model = YOLO('yolo11n.pt')
custom_model = YOLO('/content/dataset/best.pt')

print(f"ê¸°ë³¸ ëª¨ë¸ í´ë˜ìŠ¤ ìˆ˜: {len(base_model.names)}")
print(f"ì»¤ìŠ¤í…€ ëª¨ë¸ í´ë˜ìŠ¤ ìˆ˜: {len(custom_model.names)}")

# 2ï¸âƒ£ TensorRTë¡œ ë³€í™˜
print("\nâš¡ TensorRT ë³€í™˜ ì¤‘...")
print("ê¸°ë³¸ ëª¨ë¸ â†’ TensorRT ë³€í™˜...")
base_model.export(format='engine', half=True, device=0)  # FP16 ìµœì í™”
base_trt_path = 'yolo11n.engine'

print("ì»¤ìŠ¤í…€ ëª¨ë¸ â†’ TensorRT ë³€í™˜...")
custom_model.export(format='engine', half=True, device=0)
custom_trt_path = '/content/dataset/best.engine'

# 3ï¸âƒ£ TensorRT ëª¨ë¸ ë¡œë“œ
print("\nğŸ”¥ TensorRT ëª¨ë¸ ë¡œë“œ ì¤‘...")
base_trt_model = YOLO(base_trt_path)
custom_trt_model = YOLO(custom_trt_path)

print("âœ… TensorRT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

# 4ï¸âƒ£ ì˜ìƒ ë‹¤ìš´ë¡œë“œ
print("\nğŸ“¥ YouTube ì˜ìƒ ë‹¤ìš´ë¡œë“œ ì¤‘...")
!yt-dlp -f 'best[height<=720]' -o '/content/test_video.%(ext)s' 'https://www.youtube.com/watch?v=AxLmroTo3rQ'

video_path = glob.glob('/content/test_video.*')[0]
print(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {video_path}")

# 5ï¸âƒ£ ì„±ëŠ¥ ë¹„êµ í•¨ìˆ˜
def performance_comparison(video_path, frames_to_test=100):
    """PyTorch vs TensorRT ì„±ëŠ¥ ë¹„êµ"""

    print(f"\nâ±ï¸ ì„±ëŠ¥ ë¹„êµ (ì²« {frames_to_test}í”„ë ˆì„)")
    print("-" * 50)

    cap = cv2.VideoCapture(video_path)

    # PyTorch ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    pytorch_times = []
    for i in range(frames_to_test):
        ret, frame = cap.read()
        if not ret:
            break

        start_time = time.time()
        _ = base_model(frame, verbose=False)
        _ = custom_model(frame, verbose=False)
        end_time = time.time()

        pytorch_times.append(end_time - start_time)

    # TensorRT ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # ì²˜ìŒìœ¼ë¡œ ë˜ëŒë¦¬ê¸°
    tensorrt_times = []
    for i in range(frames_to_test):
        ret, frame = cap.read()
        if not ret:
            break

        start_time = time.time()
        _ = base_trt_model(frame, verbose=False)
        _ = custom_trt_model(frame, verbose=False)
        end_time = time.time()

        tensorrt_times.append(end_time - start_time)

    cap.release()

    # ê²°ê³¼ ì¶œë ¥
    pytorch_avg = np.mean(pytorch_times) * 1000  # msë¡œ ë³€í™˜
    tensorrt_avg = np.mean(tensorrt_times) * 1000
    speedup = pytorch_avg / tensorrt_avg

    print(f"ğŸ PyTorch í‰ê· : {pytorch_avg:.2f}ms/frame ({1000/pytorch_avg:.1f} FPS)")
    print(f"âš¡ TensorRT í‰ê· : {tensorrt_avg:.2f}ms/frame ({1000/tensorrt_avg:.1f} FPS)")
    print(f"ğŸš€ ì†ë„ í–¥ìƒ: {speedup:.2f}x")

    return speedup

# ì„±ëŠ¥ ë¹„êµ ì‹¤í–‰
speedup_ratio = performance_comparison(video_path)

# 6ï¸âƒ£ TensorRT ìµœì í™”ëœ ê²°í•© ì¶”ë¡ 
def tensorrt_combined_inference(video_path, output_path='/content/tensorrt_result.mp4'):
    """TensorRT ìµœì í™”ëœ ê²°í•© ì¶”ë¡ """

    cap = cv2.VideoCapture(video_path)
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    # ì¶œë ¥ ì˜ìƒ ì„¤ì •
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

    print(f"\nğŸ¬ TensorRT ìµœì í™” ì˜ìƒ ì²˜ë¦¬ ì¤‘... (ì´ {total_frames} í”„ë ˆì„)")

    frame_count = 0
    total_inference_time = 0

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        # TensorRT ì¶”ë¡  (ì‹œê°„ ì¸¡ì •)
        start_time = time.time()

        # ê¸°ë³¸ TensorRT ëª¨ë¸ ì¶”ë¡ 
        base_results = base_trt_model(frame, verbose=False)

        # ì»¤ìŠ¤í…€ TensorRT ëª¨ë¸ ì¶”ë¡ 
        custom_results = custom_trt_model(frame, verbose=False)

        inference_time = time.time() - start_time
        total_inference_time += inference_time

        # ê²°ê³¼ ì‹œê°í™”
        annotated_frame = frame.copy()

        # ê¸°ë³¸ YOLO ê²°ê³¼ ê·¸ë¦¬ê¸° (íŒŒë€ìƒ‰)
        if base_results[0].boxes is not None:
            for box in base_results[0].boxes:
                x1, y1, x2, y2 = map(int, box.xyxy[0])
                conf = float(box.conf[0])
                cls = int(box.cls[0])

                if conf > 0.3:
                    label = f"{base_trt_model.names[cls]} {conf:.2f}"
                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (255, 0, 0), 2)
                    cv2.putText(annotated_frame, label, (x1, y1-10),
                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)

        # ì»¤ìŠ¤í…€ YOLO ê²°ê³¼ ê·¸ë¦¬ê¸° (ë¹¨ê°„ìƒ‰)
        if custom_results[0].boxes is not None:
            for box in custom_results[0].boxes:
                x1, y1, x2, y2 = map(int, box.xyxy[0])
                conf = float(box.conf[0])
                cls = int(box.cls[0])

                if conf > 0.3:
                    label = f"{custom_trt_model.names[cls]} {conf:.2f}"
                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 0, 255), 2)
                    cv2.putText(annotated_frame, label, (x1, y1-10),
                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)

        # TensorRT ì •ë³´ í‘œì‹œ
        fps_text = f"TensorRT: {1/inference_time:.1f} FPS"
        cv2.putText(annotated_frame, fps_text, (10, 30),
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        out.write(annotated_frame)
        frame_count += 1

        if frame_count % 50 == 0:
            avg_fps = frame_count / total_inference_time
            print(f"   ì²˜ë¦¬ ì¤‘... {frame_count}/{total_frames} ({frame_count/total_frames*100:.1f}%) - í‰ê·  {avg_fps:.1f} FPS")

    cap.release()
    out.release()

    avg_fps = frame_count / total_inference_time
    print(f"âœ… TensorRT ê²°ê³¼ ì˜ìƒ ì €ì¥: {output_path}")
    print(f"ğŸ“Š í‰ê·  ì²˜ë¦¬ ì†ë„: {avg_fps:.1f} FPS")

    return avg_fps

# 7ï¸âƒ£ TensorRT ìµœì í™”ëœ ì¶”ë¡  ì‹¤í–‰
print("\nğŸ”¥ TensorRT ìµœì í™”ëœ ê²°í•© ì¶”ë¡  ì‹¤í–‰...")
tensorrt_fps = tensorrt_combined_inference(video_path, '/content/tensorrt_final_result.mp4')

# 8ï¸âƒ£ ê¸°ì¡´ PyTorch ì¶”ë¡ ë„ ì‹¤í–‰ (ë¹„êµìš©)
print("\nğŸ PyTorch ê¸°ì¡´ ì¶”ë¡  (ë¹„êµìš©)...")
def pytorch_combined_inference(video_path, output_path='/content/pytorch_result.mp4'):
    cap = cv2.VideoCapture(video_path)
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

    frame_count = 0
    start_time = time.time()

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        base_results = base_model(frame, verbose=False)
        custom_results = custom_model(frame, verbose=False)

        # ê°„ë‹¨í•œ ì‹œê°í™” (ì†ë„ ë¹„êµìš©)
        annotated_frame = frame.copy()
        cv2.putText(annotated_frame, "PyTorch", (10, 30),
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)

        out.write(annotated_frame)
        frame_count += 1

        if frame_count >= 100:  # 100í”„ë ˆì„ë§Œ ì²˜ë¦¬ (ë¹„êµìš©)
            break

    cap.release()
    out.release()

    total_time = time.time() - start_time
    pytorch_fps = frame_count / total_time
    return pytorch_fps

pytorch_fps = pytorch_combined_inference(video_path)

# 9ï¸âƒ£ ì„±ëŠ¥ í‰ê°€ (ì»¤ìŠ¤í…€ ëª¨ë¸)
print("\nğŸ“Š ì»¤ìŠ¤í…€ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€:")
metrics = custom_model.val(data='/content/dataset/dataset_fixed.yaml')  # custom_trt_model ëŒ€ì‹  custom_model
print(f"mAP50: {metrics.box.map50:.4f}")

# ğŸ”Ÿ ìµœì¢… ê²°ê³¼ ë° ë¹„êµ
print("\n" + "="*60)
print("ğŸ¯ ìµœì¢… ì„±ëŠ¥ ë¹„êµ ê²°ê³¼:")
print(f"ğŸ PyTorch: {pytorch_fps:.1f} FPS")
print(f"âš¡ TensorRT: {tensorrt_fps:.1f} FPS")
print(f"ğŸš€ ì „ì²´ ì†ë„ í–¥ìƒ: {tensorrt_fps/pytorch_fps:.2f}x")

print(f"\nğŸ“Š ëª¨ë¸ ì •í™•ë„ (mAP50): {metrics.box.map50:.4f}")

print("\nğŸ¬ ìµœì¢… TensorRT ê²°ê³¼ ì˜ìƒ:")
Video('/content/tensorrt_final_result.mp4', width=800)

print("\nğŸ‰ TensorRT ìµœì í™” ì™„ë£Œ!")
print("ğŸ”µ íŒŒë€ìƒ‰ ë°•ìŠ¤: ê¸°ë³¸ YOLO ê°ì²´ë“¤ (TensorRT ìµœì í™”)")
print("ğŸ”´ ë¹¨ê°„ìƒ‰ ë°•ìŠ¤: ì»¤ìŠ¤í…€ ê°ì²´ë“¤ (TensorRT ìµœì í™”)")
print("ğŸ’š ì´ˆë¡ìƒ‰ í…ìŠ¤íŠ¸: ì‹¤ì‹œê°„ FPS í‘œì‹œ")

print("\nğŸ’¾ ìƒì„±ëœ íŒŒì¼ë“¤:")
print("- tensorrt_final_result.mp4: TensorRT ìµœì í™”ëœ ìµœì¢… ê²°ê³¼")
print("- pytorch_result.mp4: PyTorch ë¹„êµìš© ê²°ê³¼")
print("- yolo11n.engine: ê¸°ë³¸ ëª¨ë¸ TensorRT ì—”ì§„")
print("- best.engine: ì»¤ìŠ¤í…€ ëª¨ë¸ TensorRT ì—”ì§„")
